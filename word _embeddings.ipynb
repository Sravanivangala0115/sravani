{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade23ff1-4b58-406b-b217-43458ba02128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sravani9\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\sravani9\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sravani9\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sravani9\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sravani9\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\sravani9\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bd0b55-2327-4e2a-addb-c56095f838d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sky nice', 'clouds nice', 'sky nice clouds nice']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sentences = ['sky is nice', 'clouds are nice', 'Sky is nice and Clouds are nice']\n",
    "\n",
    "cleaned_sentence = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    word = sentence.lower()  \n",
    "    ##lowering all the letters becaz we dont want it to treat uppercase and lower case words differently\n",
    "    \n",
    "    word = word.split()    ##splitting our sentence into words \n",
    "    \n",
    "    ##removing stop words\n",
    "    word = [i for i in word if i not in set(stopwords.words('english'))]          \n",
    "    word = \" \".join(word)               ##joining our words back to sentences\n",
    "    cleaned_sentence.append(word)       ##appending our preprocessed sentence into a new list\n",
    "    \n",
    "    \n",
    "## printing our new list\n",
    "print(cleaned_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f078e6-dc50-4e36-a249-4fcf2e4edc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1],\n",
       "       [1, 1, 0],\n",
       "       [1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features = 3)  ##give it a max features as 3\n",
    "Bagofwords = cv.fit_transform(cleaned_sentence)\n",
    "\n",
    "#Bagofwords.toarray()\n",
    "Bagofwords\n",
    "Bagofwords.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f61839a2-9a67-4bb9-81b9-0ba41df908e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'H',\n",
       " 'T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__array_priority__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__idiv__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmatmul__',\n",
       " '__rmul__',\n",
       " '__round__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '_add_dense',\n",
       " '_add_sparse',\n",
       " '_arg_min_or_max',\n",
       " '_arg_min_or_max_axis',\n",
       " '_ascontainer',\n",
       " '_asfptype',\n",
       " '_asindices',\n",
       " '_binopt',\n",
       " '_bsr_container',\n",
       " '_container',\n",
       " '_coo_container',\n",
       " '_csc_container',\n",
       " '_csr_container',\n",
       " '_deduped_data',\n",
       " '_dia_container',\n",
       " '_divide',\n",
       " '_divide_sparse',\n",
       " '_dok_container',\n",
       " '_format',\n",
       " '_get_arrayXarray',\n",
       " '_get_arrayXint',\n",
       " '_get_arrayXslice',\n",
       " '_get_columnXarray',\n",
       " '_get_index_dtype',\n",
       " '_get_intXarray',\n",
       " '_get_intXint',\n",
       " '_get_intXslice',\n",
       " '_get_sliceXarray',\n",
       " '_get_sliceXint',\n",
       " '_get_sliceXslice',\n",
       " '_get_submatrix',\n",
       " '_getcol',\n",
       " '_getmaxprint',\n",
       " '_getnnz',\n",
       " '_getrow',\n",
       " '_imag',\n",
       " '_inequality',\n",
       " '_insert_many',\n",
       " '_lil_container',\n",
       " '_major_index_fancy',\n",
       " '_major_slice',\n",
       " '_matmul_dispatch',\n",
       " '_matmul_multivector',\n",
       " '_matmul_sparse',\n",
       " '_matmul_vector',\n",
       " '_maximum_minimum',\n",
       " '_min_or_max',\n",
       " '_min_or_max_axis',\n",
       " '_minor_index_fancy',\n",
       " '_minor_reduce',\n",
       " '_minor_slice',\n",
       " '_mul_scalar',\n",
       " '_prepare_indices',\n",
       " '_process_toarray_args',\n",
       " '_raise_on_1d_array_slice',\n",
       " '_real',\n",
       " '_rmatmul_dispatch',\n",
       " '_rsub_dense',\n",
       " '_scalar_binopt',\n",
       " '_set_arrayXarray',\n",
       " '_set_arrayXarray_sparse',\n",
       " '_set_intXint',\n",
       " '_set_many',\n",
       " '_setdiag',\n",
       " '_shape',\n",
       " '_shape_as_2d',\n",
       " '_sub_dense',\n",
       " '_sub_sparse',\n",
       " '_swap',\n",
       " '_validate_indices',\n",
       " '_with_data',\n",
       " '_zero_many',\n",
       " 'arcsin',\n",
       " 'arcsinh',\n",
       " 'arctan',\n",
       " 'arctanh',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'asformat',\n",
       " 'asfptype',\n",
       " 'astype',\n",
       " 'ceil',\n",
       " 'check_format',\n",
       " 'conj',\n",
       " 'conjugate',\n",
       " 'copy',\n",
       " 'count_nonzero',\n",
       " 'data',\n",
       " 'deg2rad',\n",
       " 'diagonal',\n",
       " 'dot',\n",
       " 'dtype',\n",
       " 'eliminate_zeros',\n",
       " 'expm1',\n",
       " 'floor',\n",
       " 'format',\n",
       " 'getH',\n",
       " 'get_shape',\n",
       " 'getcol',\n",
       " 'getformat',\n",
       " 'getmaxprint',\n",
       " 'getnnz',\n",
       " 'getrow',\n",
       " 'has_canonical_format',\n",
       " 'has_sorted_indices',\n",
       " 'imag',\n",
       " 'indices',\n",
       " 'indptr',\n",
       " 'log1p',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'maxprint',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'multiply',\n",
       " 'nanmax',\n",
       " 'nanmin',\n",
       " 'ndim',\n",
       " 'nnz',\n",
       " 'nonzero',\n",
       " 'power',\n",
       " 'prune',\n",
       " 'rad2deg',\n",
       " 'real',\n",
       " 'reshape',\n",
       " 'resize',\n",
       " 'rint',\n",
       " 'set_shape',\n",
       " 'setdiag',\n",
       " 'shape',\n",
       " 'sign',\n",
       " 'sin',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'sort_indices',\n",
       " 'sorted_indices',\n",
       " 'sqrt',\n",
       " 'sum',\n",
       " 'sum_duplicates',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'toarray',\n",
       " 'tobsr',\n",
       " 'tocoo',\n",
       " 'tocsc',\n",
       " 'tocsr',\n",
       " 'todense',\n",
       " 'todia',\n",
       " 'todok',\n",
       " 'tolil',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'trunc']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(Bagofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a66721-0edd-4355-ae11-86612fb55f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_sentence</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hai</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Sentence embeddings\n",
       "0  cleaned_sentence  [1, 2, 3]\n",
       "1               hai        [1]\n",
       "2               hey        [2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d={'Sentence':['cleaned_sentence','hai','hey'],\n",
    "  'embeddings':[[1,2,3],[1],[2]]}\n",
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8c72c4-5025-4a29-9532-227ee6b8cf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sky nice</td>\n",
       "      <td>[0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clouds nice</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sky nice clouds nice</td>\n",
       "      <td>[1, 2, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Sentence embeddings\n",
       "0              sky nice  [0, 1, 1]\n",
       "1           clouds nice  [1, 1, 0]\n",
       "2  sky nice clouds nice  [1, 2, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d={'Sentence':cleaned_sentence,\n",
    "  'embeddings':Bagofwords.toarray().tolist()}\n",
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75c0661e-d7f2-4e51-ba8d-4ac95971d2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__sklearn_clone__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_build_request_for_signature',\n",
       " '_char_ngrams',\n",
       " '_char_wb_ngrams',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_check_stop_words_consistency',\n",
       " '_check_vocabulary',\n",
       " '_count_vocab',\n",
       " '_doc_link_module',\n",
       " '_doc_link_template',\n",
       " '_doc_link_url_param_generator',\n",
       " '_get_default_requests',\n",
       " '_get_doc_link',\n",
       " '_get_metadata_request',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_limit_features',\n",
       " '_more_tags',\n",
       " '_parameter_constraints',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_sort_features',\n",
       " '_stop_words_id',\n",
       " '_validate_data',\n",
       " '_validate_ngram_range',\n",
       " '_validate_params',\n",
       " '_validate_vocabulary',\n",
       " '_warn_for_unused_params',\n",
       " '_white_spaces',\n",
       " '_word_ngrams',\n",
       " 'analyzer',\n",
       " 'binary',\n",
       " 'build_analyzer',\n",
       " 'build_preprocessor',\n",
       " 'build_tokenizer',\n",
       " 'decode',\n",
       " 'decode_error',\n",
       " 'dtype',\n",
       " 'encoding',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'fixed_vocabulary_',\n",
       " 'get_feature_names_out',\n",
       " 'get_metadata_routing',\n",
       " 'get_params',\n",
       " 'get_stop_words',\n",
       " 'input',\n",
       " 'inverse_transform',\n",
       " 'lowercase',\n",
       " 'max_df',\n",
       " 'max_features',\n",
       " 'min_df',\n",
       " 'ngram_range',\n",
       " 'preprocessor',\n",
       " 'set_fit_request',\n",
       " 'set_params',\n",
       " 'set_transform_request',\n",
       " 'stop_words',\n",
       " 'strip_accents',\n",
       " 'token_pattern',\n",
       " 'tokenizer',\n",
       " 'transform',\n",
       " 'vocabulary',\n",
       " 'vocabulary_']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85cbd618-f8e6-438a-af98-1c82e68cf05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cloud</th>\n",
       "      <th>nice</th>\n",
       "      <th>sky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cloud  nice  sky\n",
       "0      1     1    1\n",
       "1      1     1    1\n",
       "2      1     0    1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(Bagofwords.toarray(),columns=['cloud','nice','sky'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05c2ad5c-7e82-4da7-a378-a159c6fce485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': 4,\n",
       " 'nice': 2,\n",
       " 'sky nice': 5,\n",
       " 'clouds': 0,\n",
       " 'clouds nice': 1,\n",
       " 'nice clouds': 3}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n",
    "# unique words with index\n",
    "# cloud is first word : first feature\n",
    "# nice is scond word: second feature\n",
    "# skt is third word: Third feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb26915f-53f3-4121-9a2e-5c49739aecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sky nice', 'clouds nice', 'sky nice clouds nice']\n",
      "{'sky': 4, 'nice': 2, 'sky nice': 5, 'clouds': 0, 'clouds nice': 1, 'nice clouds': 3}\n",
      "[[0 0 1 0 1 1]\n",
      " [1 1 1 0 0 0]\n",
      " [1 1 2 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "### All together\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = ['sky is nice', 'clouds are nice', 'Sky is nice and Clouds are nice']\n",
    "\n",
    "cleaned_sentence = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    word = sentence.lower()  \n",
    "    ##lowering all the letters becaz we dont want it to treat uppercase and lower case words differently\n",
    "    \n",
    "    word = word.split()    ##splitting our sentence into words \n",
    "    \n",
    "    ##removing stop words\n",
    "    word = [i for i in word if i not in set(stopwords.words('english'))]          \n",
    "    word = \" \".join(word)               ##joining our words back to sentences\n",
    "    cleaned_sentence.append(word)       ##appending our preprocessed sentence into a new list\n",
    "    \n",
    "    \n",
    "## printing our new list\n",
    "print(cleaned_sentence) \n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,2))  ##give it a max features as 3\n",
    "Bagofwords = cv.fit_transform(cleaned_sentence).toarray()\n",
    "\n",
    "print(cv.vocabulary_)\n",
    "print(Bagofwords)\n",
    "\n",
    "# Task for you is:  Identify the output logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66279c21-6f0d-4aa0-98ee-b2e96af54f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clouds', 'clouds nice', 'nice', 'nice clouds', 'sky', 'sky nice']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(cv.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23956c34-84a4-47c4-bd97-b3cd4bb98925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': 4,\n",
       " 'nice': 2,\n",
       " 'sky nice': 5,\n",
       " 'clouds': 0,\n",
       " 'clouds nice': 1,\n",
       " 'nice clouds': 3}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06d1680a-9fe6-4164-91db-c4cd7f2b2556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['clouds', 'sky', 'nice']\n",
      "[{'clouds': 0, 'nice': 1, 'sky': 1}, {'clouds': 1, 'nice': 1, 'sky': 0}, {'clouds': 1, 'nice': 1, 'sky': 1}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[dict_values([0, 1, 1]), dict_values([1, 1, 0]), dict_values([1, 1, 1])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################ With logic##################################\n",
    "sen=' '.join(cleaned_sentence)\n",
    "l=list(set(sen.split()))\n",
    "print(\"vocabulary:\",l)\n",
    "d={}\n",
    "l1=[]\n",
    "for sentence in cleaned_sentence:\n",
    "    for i in l:\n",
    "        if i in sentence:\n",
    "            d[i]=1\n",
    "        else:\n",
    "            d[i]=0\n",
    "    myKeys = list(d.keys())\n",
    "    myKeys.sort()\n",
    "    sorted_dict = {i: d[i] for i in myKeys}\n",
    "    l1.append(sorted_dict)\n",
    "\n",
    "print(l1)\n",
    "l2=[i.values() for i in l1]\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c88890fd-3f3f-4f94-8b5d-8fc9d5bb51f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sky': 1, 'is': 2, 'nice': 4, 'clouds': 1, 'are': 2, 'Sky': 1, 'and': 1, 'Clouds': 1}\n"
     ]
    }
   ],
   "source": [
    "# Creating word histogram\n",
    "import nltk\n",
    "word2count = {}\n",
    "for data in sentences:\n",
    "    words = nltk.word_tokenize(data) # we are split into words\n",
    "    for word in words:               # we are calling each word\n",
    "        if word not in word2count.keys(): # if the word not in dictionary, we are \n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "print(word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029bbafe-43c4-4c7b-a497-58a29bc2ac2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
